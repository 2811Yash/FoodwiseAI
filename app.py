from PIL import Image
from transformers import AutoModelForCausalLM, AutoTokenizer
import streamlit as st
from PIL import Image
from huggingface_hub import login
import os

hf_token = os.getenv("HF_TOKEN")
"""
    The function `load_model_and_tokenizer` loads a pre-trained model and tokenizer for a causal
    language modeling task using Hugging Face's Transformers library.
    :return: The `load_model_and_tokenizer` function returns a tuple containing the model and tokenizer
    loaded from the specified model ID and revision.
    """
@st.cache_resource
def load_model_and_tokenizer():
    model_id = "vikhyatk/moondream2"
    revision = "2024-07-23"

    model = AutoModelForCausalLM.from_pretrained(
            model_id, trust_remote_code=True, revision=revision)

    tokenizer = AutoTokenizer.from_pretrained(model_id, revision=revision)
    return model, tokenizer



model, tokenizer = load_model_and_tokenizer()

if "messages" not in st.session_state:
    st.session_state.messages=[]

    """
        The `query` function takes an image and a prompt, encodes the image, asks a question based on the
        prompt, and returns the description generated by the model
    """
def query(image, prompt):
    print("start")
    enc_image = model.encode_image(image)
    description = model.answer_question(enc_image, prompt, tokenizer)
    print("done")
    return description



st.title("FoodwiseAI chatbot ðŸ¤–")

# The line `uploaded_file = st.file_uploader("Choose an image")` is creating a file uploader widget using Streamlit. This widget allows users to upload an image file from their local system.
uploaded_file = st.file_uploader("Choose an image")

if uploaded_file is not None:
    image_data = uploaded_file.read()
    st.image(image_data)
    st.write("file uploaded")
    image = Image.open(uploaded_file)
    # Specify the file path to save the image
    filepath = "./uploaded_image.jpg" 
    # Save the image
    image.save(filepath)
    st.success(f"Image saved successfully at {filepath}")

    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message['content'])
    
    # The line `prompt = st.chat_input("enter your prompt")` is creating a chat input widget using Streamlit. This widget allows the user to enter a prompt or message in a chat-like interface.
    # The text entered by the user in this chat input will be used as the prompt for the chatbot to generate a response based on the uploaded image.
    prompt = st.chat_input("enter your prompt")
    if prompt :
        with st.chat_message("user"):
            st.markdown(prompt)
        
        # This line adding a new message to the `messages` list in the `session_state` of the Streamlit application.
        st.session_state.messages.append({'role':'user','content' : prompt})

        # The line `response=query(image,prompt)` is calling the `query` function with the `image` and `prompt` as input parameters. This function takes an image and a prompt, encodes the image, asks a question based on the prompt, and returns the description generated by the model. In this case, it generates a response based on the uploaded image and the prompt entered by the user in the chat interface.
        response=query(image,prompt) 
        with st.chat_message("assistant"):
            st.markdown(response)
        # This line adding a new message to the `messages` list in the `session_state` of the Streamlit application.
        st.session_state.messages.append({"role":"assistant","content": response})
